---
title: "Weight Lifting prediction"
author: "Pamela Chua"
date: "26 October, 2014"
output: html_document
---

In this prediction exercise, we used the dataset collected using devices such as Jawbone Up, Nike FuelBand and Fitbit to predict the manner in which the subjects did the weight lifting exercise. In order to build the model, we first needed to understand the data and conduct exploratory data analysis, which led us to preprocess the data into a more tidy data frame.

In terms of the model chosen, we picked random forests as the algorithm is usually one of the top few peformers. The low out-of-bag error estimate (0.32%) of the model validated this. We then used this model built using random forests to predict with our test set.

### Loading the data
```{r,cache=TRUE}
# Read in the data sets
train <- read.csv("pml-training.csv", na.strings=c("#DIV/0!", "", " ", "NA"))
test <- read.csv("pml-testing.csv", na.strings=c("#DIV/0!", "", " ", "NA"))
```

### Exploratory data analysis
```{r}
# Looking and understanding the data variables
str(train)
table(train$classe)
```

We have 160 variables but from the above exploration, it seems that there are a number of variables with NA values. We will remove the columns where most (> 90%) of the observations are NA. Also, we note that the first 7 columns are variables like id number, user name, time stamp, etc which are not relevant to our prediction so we will remove these columns as well.

```{r}
retainCols <- which(colSums(is.na(train))/nrow(train) <= 0.1)
train_pp <- train[,retainCols]
train_pp <- train_pp[,-c(1:7)]

# Do the same for the test set since we are not using the columns for prediction
test_pp <- test[,retainCols[-60]]
test_pp <- test_pp[,-c(1:7)]
```

Now we are left with 52 predictor variables and a more tidy data set to model the prediction.

### Model building using Random Forests
```{r,message=FALSE}
set.seed(1)
library(randomForest)
library(caret)
modFit <- randomForest(classe ~ ., data=train_pp)
modFit
```

For random forests, there is no need for extra cross-validation or a separate test set to get an unbiased estimate of the test set error. This is because it is already estimated internally in the algorithm. From the model fit, we get an out-of-bag (OOB) estimate of error rate of 0.32% which is good. Hence we can use this model to now apply to our test set.

### Model prediction using Random Forests
We now use our model to predict the test set.
```{r}
pred <- predict(modFit, test_pp)
pred
```

### Conclusion
In order to build this prediction model for the weight lifting exercise, we first explored, pre-processed the data, fitted the model, verified using cross validation and finally used our model to predict on our test set. We estimate the out-of-sample error to be quite low, based on the OOB error from our model.